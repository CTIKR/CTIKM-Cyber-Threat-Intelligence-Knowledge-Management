{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from stanza.server import CoreNLPClient\n",
    "import glob\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from fuzzywuzzy import fuzz\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from simpletransformers.ner import NERModel\n",
    "import re\n",
    "import spacy\n",
    "from iocparser import IOCParser\n",
    "import scipy\n",
    "import neuralcoref\n",
    "import sys\n",
    "import stanza.protobuf.CoreNLP_pb2 as CoreNLP_pb2\n",
    "from nltk.corpus import words\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "\n",
    "word_set = set(words.words())\n",
    "sys.modules['CoreNLP_pb2'] = CoreNLP_pb2\n",
    "os.environ[\"CORENLP_HOME\"] = 'C:/Users/H/Documents/stanford-corenlp-4.5.1'\n",
    "w2v = pickle.load(open('pkl/w2v_ref_blog_lemma.pkl', 'rb'))\n",
    "nlp = spacy.load('en_core_web_lg', exclude=['ner', 'lemmatizer'])\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "mitre_groups = pickle.load(open('pkl/mitre_groups.pkl', 'rb'))\n",
    "mitre_malwares = pickle.load(open('pkl/mitre_malwares.pkl', 'rb'))\n",
    "programm_language = pickle.load(open('pkl/programming_language.pkl', 'rb'))\n",
    "\n",
    "mitre_groups = set(list(map(str.lower, mitre_groups)))\n",
    "mitre_malwares = set(list(map(str.lower, mitre_malwares)))\n",
    "\n",
    "malware = pickle.load(open('pkl/malware.pkl', 'rb'))\n",
    "software = pickle.load(open('pkl/software.pkl', 'rb'))\n",
    "actor = pickle.load(open('pkl/actor.pkl', 'rb'))\n",
    "\n",
    "mal_suffix = ['rat', 'downloader', 'trojan', 'ransomware', 'malware', 'virus', 'worm', 'backdoor']\n",
    "for malname in list(malware):\n",
    "    if ' ' in malname:\n",
    "        for _ in mal_suffix:\n",
    "            if _ in malname.split(' '):\n",
    "                malware.remove(malname)\n",
    "                malware.add(''.join(filter(lambda x: x not in mal_suffix, malname.split(' '))))\n",
    "for malname in list(malware):\n",
    "    if malname in word_set:\n",
    "        malware.remove(malname)\n",
    "\n",
    "for softname in list(software):\n",
    "    if softname in word_set:\n",
    "        software.remove(softname)\n",
    "\n",
    "for actname in list(actor):\n",
    "    if actname in word_set:\n",
    "        actor.remove(actname)\n",
    "\n",
    "\n",
    "class Triple():\n",
    "    def __init__(self, sub, rel, obj, sub_ent='', rel_ent='', obj_ent='', sub_ent_conf=0, obj_ent_conf=0, conf=0):\n",
    "        self.sub = sub.lower()\n",
    "        self.rel = rel.lower()\n",
    "        self.obj = obj.lower()\n",
    "        self.sub_ent = sub_ent\n",
    "        self.rel_ent = rel_ent\n",
    "        self.obj_ent = obj_ent\n",
    "        self.sub_ent_conf = sub_ent_conf\n",
    "        self.obj_ent_conf = obj_ent_conf\n",
    "        self.confidence = conf\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.simple_sub + self.simple_rel + self.simple_obj)\n",
    "\n",
    "    def set_raw_article_doc(self, doc):\n",
    "        self.article_doc = doc\n",
    "\n",
    "    def set_raw_sent(self, sent):\n",
    "        self.raw_sent = sent\n",
    "\n",
    "    def set_raw_triple(self, triple_obj):\n",
    "        self.triple_obj = triple_obj\n",
    "\n",
    "    def set_sent_ind(self, sent_ind):\n",
    "        self.sent_ind = sent_ind\n",
    "\n",
    "    def set_article_ref(self, article):\n",
    "        self.article_id = article\n",
    "\n",
    "    def set_tactic(self, tactic):\n",
    "        self.tactic = tactic\n",
    "\n",
    "    def set_tactic_conf(self, tactic_conf):\n",
    "        self.tactic_conf = tactic_conf\n",
    "\n",
    "    def set_behave_conf(self, behave_conf):\n",
    "        self.behave_conf = behave_conf\n",
    "\n",
    "    def set_relative_sent_ind(self, relative_sent_order):\n",
    "        self.relative_sent_order = relative_sent_order\n",
    "\n",
    "    def set_count(self, count):\n",
    "        self.count = count\n",
    "\n",
    "    def add_cluster(self, cluster):\n",
    "        self.from_cluster = set()\n",
    "        self.from_cluster.add(cluster)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        sub_eq = False\n",
    "        if self.sub in other.sub or other.sub in self.sub:\n",
    "            sub_eq = True\n",
    "        elif fuzz.ratio(self.sub, other.sub) > 90:\n",
    "            sub_eq = True\n",
    "        else:\n",
    "            self_sub_tokens = self.sub.split(' ')\n",
    "            other_sub_tokens = other.sub.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_sub_tokens)) and all(map(lambda x: x in w2v.wv, other_sub_tokens)):\n",
    "                self_sub_vec = np.mean([w2v.wv[x] for x in self_sub_tokens], axis=0)\n",
    "                other_sub_vec = np.mean([w2v.wv[x] for x in other_sub_tokens], axis=0)\n",
    "                sub_eq = spatial.distance.cosine(self_sub_vec, other_sub_vec) < 0.3\n",
    "\n",
    "        rel_eq = False\n",
    "        if self.rel in other.rel or other.rel in self.rel:\n",
    "            rel_eq = True\n",
    "        elif self.rel == other.rel:\n",
    "            rel_eq = True\n",
    "        else:\n",
    "            self_rel_tokens = self.rel.split(' ')\n",
    "            other_rel_tokens = other.rel.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_rel_tokens)) and all(map(lambda x: x in w2v.wv, other_rel_tokens)):\n",
    "                self_rel_vec = np.mean([w2v.wv[x] for x in self_rel_tokens], axis=0)\n",
    "                other_rel_vec = np.mean([w2v.wv[x] for x in other_rel_tokens], axis=0)\n",
    "                rel_eq = spatial.distance.cosine(self_rel_vec, other_rel_vec) < 0.3\n",
    "\n",
    "        obj_eq = False\n",
    "        if self.obj in other.obj or other.obj in self.obj:\n",
    "            obj_eq = True\n",
    "        elif fuzz.ratio(self.obj, other.obj) > 90:\n",
    "            obj_eq = True\n",
    "        else:\n",
    "            self_obj_tokens = self.obj.split(' ')\n",
    "            other_obj_tokens = other.obj.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_obj_tokens)) and all(map(lambda x: x in w2v.wv, other_obj_tokens)):\n",
    "                self_obj_vec = np.mean([w2v.wv[x] for x in self_obj_tokens], axis=0)\n",
    "                other_obj_vec = np.mean([w2v.wv[x] for x in other_obj_tokens], axis=0)\n",
    "                obj_eq = spatial.distance.cosine(self_obj_vec, other_obj_vec) < 0.3\n",
    "\n",
    "        return sub_eq and rel_eq and obj_eq and self.sub_ent == other.sub_ent and self.obj_ent == other.obj_ent and self.rel_ent == other.rel_ent\n",
    "\n",
    "    def print(self):\n",
    "        print(f'{self.sub}({self.sub_ent}) >> {self.rel} >> {self.obj}({self.obj_ent})')\n",
    "\n",
    "\n",
    "def bert_ner(ann):\n",
    "    tokenized = [[x.value for x in sent.token] for sent in ann.sentence]\n",
    "    predictions, raw_outputs = model.predict(tokenized, split_on_space=False)\n",
    "    return predictions, raw_outputs\n",
    "\n",
    "\n",
    "def simple_match(candidate, ioc_dic):\n",
    "    candidate = candidate.lower().strip()\n",
    "    if (candidate.startswith(\"cve\")):\n",
    "        return \"CVE\"\n",
    "    for _ in mitre_groups:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Actor\"\n",
    "    for _ in mitre_malwares:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Malware\"\n",
    "    for _ in programm_language:\n",
    "        if _ == candidate:\n",
    "            return \"Programming_Language\"\n",
    "    for _ in ioc_dic:\n",
    "        if _ in candidate:\n",
    "            textobj = IOCParser(ioc_dic[_]);\n",
    "            results = textobj.parse();\n",
    "            if len(results) > 0:\n",
    "                return results[0].kind\n",
    "    return ''\n",
    "\n",
    "\n",
    "def process_triple_tokens(tokens, ann, predictions, raws, ioc_dic):\n",
    "    redundant = ['PRP$', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'DT']\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    sub_tokens = tokens\n",
    "    simplified_sub_tokens = []\n",
    "    sub_entity = ''\n",
    "    sub_ner_conf = 0\n",
    "    for sub_token in sub_tokens:\n",
    "        token_obj = ann.sentence[sub_token.sentenceIndex].token[sub_token.tokenIndex]\n",
    "        if token_obj.pos.startswith('NN'):\n",
    "            ner_tag = 'O'\n",
    "            try:\n",
    "                ner_tag = list(predictions[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0]\n",
    "                sub_ner_conf = max(\n",
    "                    scipy.special.softmax(list(raws[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0][0]))\n",
    "            except:\n",
    "                pass\n",
    "            if ner_tag.startswith('B'):\n",
    "                sub_entity = ner_tag.split('-')[1]\n",
    "            if token_obj.pos in inflex:\n",
    "                simplified_sub_tokens.append(token_obj.lemma)\n",
    "            else:\n",
    "                simplified_sub_tokens.append(token_obj.word)\n",
    "        elif token_obj.pos not in redundant:\n",
    "            simplified_sub_tokens.append(token_obj.word)\n",
    "    if len(simplified_sub_tokens) == 0:\n",
    "        return None, None, None\n",
    "    else:\n",
    "        sub = \" \".join(simplified_sub_tokens)\n",
    "        simple_ent = simple_match(sub, ioc_dic)\n",
    "        if simple_ent != '':\n",
    "            sub_entity = simple_ent\n",
    "            sub_ner_conf = 1\n",
    "    return sub, sub_entity, sub_ner_conf\n",
    "\n",
    "\n",
    "def extract_triples_with_ner(ann, predictions, raws, ioc_dic, doc_id=None):\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ', 'NNS', 'NNPS']\n",
    "    extracted = list()\n",
    "    sub_counter = Counter()\n",
    "    obj_counter = Counter()\n",
    "    relation_counter = Counter()\n",
    "    for sent_ind, sent in enumerate(ann.sentence):\n",
    "        triples = sent.openieTriple\n",
    "        for triple in triples:\n",
    "            print(triple)\n",
    "            sub_tokens = triple.subjectTokens\n",
    "            sub, sub_entity, sub_ner_conf = process_triple_tokens(sub_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not sub:\n",
    "                continue\n",
    "\n",
    "            obj_tokens = triple.objectTokens\n",
    "            obj, obj_entity, obj_ner_conf = process_triple_tokens(obj_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not obj:\n",
    "                continue\n",
    "\n",
    "            rel_tokens = triple.relationTokens\n",
    "            simplified_rel_tokens = []\n",
    "            for rel_token in rel_tokens:\n",
    "                token_obj = ann.sentence[rel_token.sentenceIndex].token[rel_token.tokenIndex]\n",
    "                if token_obj.pos != 'RB':\n",
    "                    if token_obj.pos in inflex:\n",
    "                        simplified_rel_tokens.append(token_obj.lemma)\n",
    "                    else:\n",
    "                        simplified_rel_tokens.append(token_obj.word)\n",
    "            if len(simplified_rel_tokens) == 0:\n",
    "                print(\"no rel\")\n",
    "                continue\n",
    "            else:\n",
    "                rel = \" \".join(simplified_rel_tokens)\n",
    "\n",
    "            for k, v in ioc_dic.items():\n",
    "                if k in sub:\n",
    "                    sub = sub.replace(k, v)\n",
    "                if k in obj:\n",
    "                    obj = obj.replace(k, v)\n",
    "                if k in rel:\n",
    "                    rel = rel.replace(k, v)\n",
    "\n",
    "            tri = Triple(sub, rel, obj, sub_ent=sub_entity, obj_ent=obj_entity, sub_ent_conf=sub_ner_conf,\n",
    "                         obj_ent_conf=obj_ner_conf, conf=triple.confidence)\n",
    "            tri.set_sent_ind(sent_ind)\n",
    "            tri.set_relative_sent_ind(sent_ind / len(ann.sentence))\n",
    "            tri.set_article_ref(doc_id)\n",
    "            tri.set_raw_triple(triple)\n",
    "            extracted.append(tri)\n",
    "            sub_counter.update([sub])\n",
    "            obj_counter.update([obj])\n",
    "            relation_counter.update([rel])\n",
    "    return extracted, (sub_counter, obj_counter), relation_counter\n",
    "\n",
    "\n",
    "def change_ioc(text):\n",
    "    count = 0\n",
    "    dic = dict()\n",
    "    textobj = IOCParser(text);\n",
    "    results = textobj.parse();\n",
    "    for res in results:\n",
    "        sig = res.kind + str(count)\n",
    "        dic[sig] = res.value\n",
    "        count += 1\n",
    "        text = text.replace(res.value, sig)\n",
    "    return text, dic\n",
    "\n",
    "tactic_dic = {\n",
    "    0: 'Initial Access', \n",
    "    1: 'Execution', \n",
    "    2: 'Defense Evasion', \n",
    "    3: 'Command and Control', \n",
    "    4: 'Privilege Escalation', \n",
    "    5: 'Persistence', \n",
    "    6: 'Lateral Movement', \n",
    "    7: 'DataLeak', \n",
    "    8: 'Exfiltration', \n",
    "    9: 'Impact'\n",
    "}\n",
    "\n",
    "\n",
    "valid_ner = [\n",
    "         'Known_Malware',\n",
    "         'PRO',\n",
    "         'PERSON',\n",
    "         'filename',\n",
    "         'uri',\n",
    "         'md5',\n",
    "         'ACTOR',\n",
    "         'Known_Actor',\n",
    "         'sha256',\n",
    "         'URL',\n",
    "         'sha1',\n",
    "         'CVE',\n",
    "]\n",
    "\n",
    "def dedupe(article_extracts):\n",
    "    seen_sent = dict()\n",
    "    unique = []\n",
    "    for triple in article_extracts:\n",
    "        sent_ind = triple.sent_ind\n",
    "        if sent_ind not in seen_sent:\n",
    "            seen_sent[sent_ind] = list()\n",
    "        \n",
    "        dup = False\n",
    "        for seen_triple in seen_sent[sent_ind]:\n",
    "            if seen_triple == triple:\n",
    "                if len(seen_triple.sub) >= len(triple.sub) and len(seen_triple.obj) >= len(triple.obj):\n",
    "                    seen_sent[sent_ind].remove(seen_triple)\n",
    "                    dup = False\n",
    "                else:\n",
    "                    #seen_triple.set_count(seen_triple.count+1)\n",
    "                    dup = True\n",
    "                break\n",
    "                    \n",
    "        if not dup:\n",
    "            seen_sent[sent_ind].append(triple)\n",
    "    \n",
    "    for k, v in seen_sent.items():\n",
    "        unique += v\n",
    "        \n",
    "    return unique\n",
    "  \n",
    "def fusion(trip):\n",
    "    det_ner = ['filename', 'sha256', 'CVE', 'uri', 'md5', 'sha1']\n",
    "    if trip.sub_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub = single\n",
    "                trip.sub_ent = 'Known_Malware'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "           # if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'PRO'\n",
    "                trip.sub_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.sub:\n",
    "            if single in trip.sub:\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "            elif single in trip.sub.split(' '):\n",
    "                trip.sub_ent = 'Known_Actor'\n",
    "                trip.sub_ent_conf = 1\n",
    "    \n",
    "    if trip.obj_ent not in det_ner:\n",
    "        for single in malware:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj = single\n",
    "                trip.obj_ent = 'Known_Malware'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in software:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'PRO'\n",
    "                trip.obj_ent_conf = 1\n",
    "                \n",
    "        for single in actor:\n",
    "            #if ' ' in single and single in trip.obj:\n",
    "            if single in trip.obj:\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "            elif single in trip.obj.split(' '):\n",
    "                trip.obj_ent = 'Known_Actor'\n",
    "                trip.obj_ent_conf = 1\n",
    "\n",
    "def filter_ner(triples):\n",
    "    filtered = []\n",
    "    for triple in triples:\n",
    "        if triple.sub_ent != '' and triple.sub_ent in valid_ner and triple.obj_ent != '' and triple.obj_ent in valid_ner:\n",
    "            filtered.append(triple)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "def filter_conf(triples, behav_thresh=0.5):\n",
    "    filtered = []\n",
    "    for triple in triples:        \n",
    "        if triple.confidence < 0.8:\n",
    "            continue\n",
    "            \n",
    "        if triple.behave_conf < behav_thresh:\n",
    "            continue\n",
    "        \n",
    "        filtered.append(triple)\n",
    "    return filtered\n",
    "\n",
    "def display_triples(triples_list):\n",
    "    for e in triples_list:\n",
    "        #print(e.sub,'|',e.rel,'|',e.obj)\n",
    "        print(e.sub,'|',e.rel,'|',e.obj,'|',e.sub_ent,'|',e.obj_ent,'|',e.sub_ent_conf,'|',e.obj_ent_conf,'|',e.confidence)\n",
    "\n",
    "\n",
    "def build_graph(triples):\n",
    "    g = nx.MultiDiGraph()\n",
    "    for triple in triples:\n",
    "        if triple.sub not in g.nodes:\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_node(triple.sub, appearance=1, ent=triple.sub_ent, mentions=s)\n",
    "        else:\n",
    "            g.nodes[triple.sub]['appearance'] += 1\n",
    "            g.nodes[triple.sub]['mentions'].append(triple)\n",
    "\n",
    "        if triple.obj not in g.nodes:\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_node(triple.obj, appearance=1, ent=triple.obj_ent, mentions=s)\n",
    "        else:\n",
    "            g.nodes[triple.obj]['appearance'] += 1\n",
    "            g.nodes[triple.obj]['mentions'].append(triple)\n",
    "\n",
    "        if not g.has_edge(triple.sub, triple.obj):\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_edge(triple.sub, triple.obj, appearance=1, relation=triple.rel, mentions=s)\n",
    "        elif g[triple.sub][triple.obj][0]['relation'] == triple.rel:\n",
    "            g[triple.sub][triple.obj][0]['appearance'] += 1\n",
    "            g[triple.sub][triple.obj][0]['mentions'].append(triple)\n",
    "        else:\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_edge(triple.sub, triple.obj, appearance=1, relation=triple.rel, mentions=s)\n",
    "    return g\n",
    "\n",
    "def extract_subgraph(g, nodes):\n",
    "    sg = g.subgraph(nodes)\n",
    "    sg_tactics = set()\n",
    "    sg_articles = set()\n",
    "    sg_linking_nodes = set()\n",
    "    \n",
    "    edge_labels = {}\n",
    "    edge_mentions = []\n",
    "    node2article = dict()\n",
    "    for edge in sg.edges(data=True):\n",
    "        mentions = edge[2]['mentions']\n",
    "        sg_articles.update(map(lambda x: x.article_id, mentions))\n",
    "        for mention in mentions:\n",
    "            for tac in mention.tactic:\n",
    "                sg_tactics.add(tac)\n",
    "        if edge[0] not in node2article:\n",
    "            node2article[edge[0]] = set()\n",
    "        node2article[edge[0]].update(map(lambda x: x.article_id, mentions))\n",
    "        if edge[1] not in node2article:\n",
    "            node2article[edge[1]] = set()\n",
    "        node2article[edge[1]].update(map(lambda x: x.article_id, mentions))\n",
    "        \n",
    "    for k, v in node2article.items():\n",
    "        if len(v)>1:\n",
    "            sg_linking_nodes.add(k)\n",
    "            \n",
    "    return sg, sg_tactics, sg_articles, sg_linking_nodes\n",
    "\n",
    "def draw_graph(g, scheme=False, legend=False):\n",
    "    node_deg = nx.degree(g)\n",
    "    layout = nx.spring_layout(g, k=2, iterations=80)\n",
    "    plt.figure(num=None, figsize=(50, 50), dpi=80)\n",
    "\n",
    "    if scheme:\n",
    "        node_labels = {node[0] : f\"{node[0]}\" for node in g.nodes(data=True)}\n",
    "    else:\n",
    "        node_labels = {node[0] : f\"{node[0]}\\n({node[1]['ent']})\" for node in g.nodes(data=True)}\n",
    "    nx.draw_networkx(\n",
    "        g,\n",
    "        node_size=[int(deg[1]) * 1000 for deg in node_deg],\n",
    "        arrowsize=10,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        node_shape=\"s\",\n",
    "        bbox=dict(facecolor=\"skyblue\", edgecolor='black', boxstyle='round,pad=0.1'),\n",
    "        node_color='white',\n",
    "        font_size=13,\n",
    "        with_labels=True,\n",
    "        labels = node_labels,\n",
    "        )\n",
    "\n",
    "    edge_labels = {}\n",
    "    edge_mentions = []\n",
    "    for edge in g.edges(data=True):\n",
    "        mentions = edge[2]['mentions']\n",
    "        mention_articles = ','.join(map(lambda x: str(x.article_id), mentions))\n",
    "        mention_tactics = set()\n",
    "        for mention in mentions:\n",
    "            if len(mention.tactic)>0:\n",
    "                for tac in mention.tactic: \n",
    "                    mention_tactics.add(tactic_dic[tac])\n",
    "        mention_tactics = '('+','.join(mention_tactics)+')'\n",
    "        \n",
    "        edge_labels[(edge[0], edge[1])] = '-'.join([mention_tactics, edge[2]['relation'], mention_articles])\n",
    "        edge_mentions.append(mentions)\n",
    "    edge_mentions = [item for sublist in edge_mentions for item in sublist]\n",
    "    descriptions = set([x.article_id for x in edge_mentions])\n",
    "    \n",
    "    #texts = ['::'.join([str(x), articles.loc[x]['title'], articles.loc[x]['date']]) for x in descriptions]\n",
    "    nx.draw_networkx_edge_labels(g, \n",
    "                                 pos=layout, \n",
    "                                 edge_labels=edge_labels,\n",
    "                                 font_color='red',\n",
    "                                 font_size=12)\n",
    "    #if legend:\n",
    "        #plt.legend(texts, fontsize=10)\n",
    "\n",
    "model = NERModel(\"roberta\", \"nermodel\",use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [01:15<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "df=pd.read_csv('top10_tor_data_frame.csv')\n",
    "\n",
    "#find article and resolve coref\n",
    "coref_resolved_df=pd.DataFrame(columns=['hardcode_id','line'])\n",
    "#for i in df.hardcode_id.unique(): add tqdm\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(df.hardcode_id.unique()):\n",
    "    chunk_df=df[df.hardcode_id==i]\n",
    "    fulltext='<SEP>'.join(chunk_df.line.values)\n",
    "    fulltext = nlp(fulltext)._.coref_resolved\n",
    "    for line in fulltext.split('<SEP>'):\n",
    "        coref_resolved_df=coref_resolved_df.append({'hardcode_id':i,'line':line},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv, encoding='utf-8-sig' to avoid the problem of Chinese characters, index=False to avoid the first column of index\n",
    "coref_resolved_df.to_csv('tor_top10_coref_resolved.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CoreNLPClient(\n",
    "        properties={'annotators': 'tokenize,mwt,ssplit,pos,lemma,ner,parse,depparse,natlog,openie',\n",
    "                            'coref.algorithm' : 'neural',\n",
    "                            'ssplit.boundaryTokenRegex': '''''',\n",
    "                            'openie.resolve_coref': False, \n",
    "                            'openie.triple.strict': True, \n",
    "                            'openie.triple.all_nominals':False,\n",
    "                            'openie.affinity_probability_cap': 1.0},\n",
    "        timeout=30000,\n",
    "        memory='4G',endpoint=\"http://localhost:9079\") as client:    \n",
    "\n",
    "        for doc_id, text in tzip(doc_ids, texts):\n",
    "            text, ioc_dic = change_ioc(text)\n",
    "            text = nlp(text)._.coref_resolved;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent_ind, sent in enumerate(ann.sentence):\n",
    "    print(sent_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=coref_resolved_df.line.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract triples\n",
    "extracted_list_list=[]\n",
    "entity_count_list=[]\n",
    "rel_count_list=[]\n",
    "with CoreNLPClient(\n",
    "        properties={'annotators': 'tokenize,mwt,ssplit,pos,lemma,ner,parse,depparse,natlog,openie',\n",
    "                            'coref.algorithm' : 'neural',\n",
    "                            'ssplit.boundaryTokenRegex': '''''',\n",
    "                            'openie.resolve_coref': False, \n",
    "                            'openie.triple.strict': True, \n",
    "                            'openie.triple.all_nominals':False,\n",
    "                            'openie.affinity_probability_cap': 1.0},\n",
    "        timeout=30000,\n",
    "        memory='4G',endpoint=\"http://localhost:9078\") as client:\n",
    "    for text in sentences:\n",
    "        text, ioc_dic = change_ioc(text)\n",
    "        ann = client.annotate(text)\n",
    "        ner, raw_out = bert_ner(ann)      \n",
    "        extracted_list, entity_count, rel_count = extract_triples_with_ner(ann, ner, raw_out, ioc_dic, doc_id = 1)\n",
    "        extracted_list_list.append(extracted_list)\n",
    "        entity_count_list.append(entity_count)\n",
    "        rel_count_list.append(rel_count)\n",
    "print('done');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show results\n",
    "for extracted_list in extracted_list_list:\n",
    "    for e in extracted_list:\n",
    "        if e.sub_ent!='' or e.obj_ent!='':\n",
    "            print(e.sub,'|',e.rel,'|',e.obj,'|',e.sub_ent,'|',e.obj_ent,'|',e.sub_ent_conf,'|',e.obj_ent_conf,'|',e.confidence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b20541ee1c7841e3bda15bbcdc17ee50b92195b8987f888b2641cbc25b3974b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
