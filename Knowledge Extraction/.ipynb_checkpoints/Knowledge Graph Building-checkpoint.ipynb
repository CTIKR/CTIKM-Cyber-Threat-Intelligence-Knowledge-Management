{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd1330b-be22-4262-80cd-774ff406b631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 07:21:09,089 - INFO - Loading model from /home/lcl/.neuralcoref_cache/neuralcoref\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from stanza.server import CoreNLPClient\n",
    "import glob\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from fuzzywuzzy import fuzz\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from simpletransformers.ner import NERModel\n",
    "import re\n",
    "import spacy\n",
    "from iocparser import IOCParser\n",
    "import scipy\n",
    "import neuralcoref\n",
    "import sys\n",
    "import stanza.protobuf.CoreNLP_pb2 as CoreNLP_pb2\n",
    "\n",
    "sys.modules['CoreNLP_pb2'] = CoreNLP_pb2\n",
    "os.environ[\"CORENLP_HOME\"] = '/home/lcl/corenlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a8c847-8255-44cd-9a6e-2be0f2bf36f0",
   "metadata": {},
   "source": [
    "# Prepare Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f518ab-1313-4aed-8d2e-ddddb1b49cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = pickle.load(open('../../src/transformer/w2v_ref_blog_lemma.pkl','rb'))\n",
    "model = NERModel(\n",
    "    \"roberta\", \"../../ner/outputs/best_model/\"\n",
    ")\n",
    "nlp = spacy.load('en_core_web_lg', exclude=['ner', 'lemmatizer'])\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf48ae67-a080-4536-922e-895f14db87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitre_groups = pickle.load(open('../../ner/mitre_groups.pkl', 'rb'))\n",
    "mitre_malwares = pickle.load(open('../../ner/mitre_malwares.pkl', 'rb'))\n",
    "programm_language = pickle.load(open('../../ner/programming_language.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3db5e2-12fd-4abe-b026-9a354d3fac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitre_groups = set(list(map(str.lower, mitre_groups)))\n",
    "mitre_malwares = set(list(map(str.lower, mitre_malwares)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ccc18b0-76d0-4474-8741-70b57f9e9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for malname in list(mitre_malwares):\n",
    "    if malname in nlp.vocab.strings:\n",
    "        mitre_malwares.remove(malname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f66b37e-8717-4b8d-a4e7-48d6d6b00b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triple():\n",
    "    def __init__(self, sub, rel, obj, sub_ent='', rel_ent='', obj_ent='', sub_ent_conf=0, obj_ent_conf=0, conf=0):\n",
    "        self.sub = sub.lower()\n",
    "        self.rel = rel.lower()\n",
    "        self.obj = obj.lower()\n",
    "        self.sub_ent = sub_ent\n",
    "        self.rel_ent = rel_ent\n",
    "        self.obj_ent = obj_ent\n",
    "        self.sub_ent_conf = sub_ent_conf\n",
    "        self.obj_ent_conf = obj_ent_conf\n",
    "        self.confidence = conf\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.simple_sub+self.simple_rel+self.simple_obj)\n",
    "    \n",
    "    def set_raw_article_doc(self, doc):\n",
    "        self.article_doc = doc\n",
    "    \n",
    "    def set_raw_sent(self, sent):\n",
    "        self.raw_sent = sent\n",
    "        \n",
    "    def set_raw_triple(self, triple_obj):\n",
    "        self.triple_obj = triple_obj\n",
    "        \n",
    "    def set_sent_ind(self, sent_ind):\n",
    "        self.sent_ind = sent_ind\n",
    "        \n",
    "    def set_article_ref(self, article):\n",
    "        self.article_id = article\n",
    "        \n",
    "    def set_tactic(self, tactic):\n",
    "        self.tactic = tactic\n",
    "        \n",
    "    def set_tactic_conf(self, tactic_conf):\n",
    "        self.tactic_conf = tactic_conf\n",
    "        \n",
    "    def set_behave_conf(self, behave_conf):\n",
    "        self.behave_conf = behave_conf\n",
    "        \n",
    "    def set_relative_sent_ind(self, relative_sent_order):\n",
    "        self.relative_sent_order = relative_sent_order\n",
    "        \n",
    "    def set_count(self, count):\n",
    "        self.count = count\n",
    "        \n",
    "    def add_cluster(self, cluster):\n",
    "        self.from_cluster = set()\n",
    "        self.from_cluster.add(cluster)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        sub_eq = False\n",
    "        if self.sub in other.sub or other.sub in self.sub:\n",
    "            sub_eq = True\n",
    "        elif fuzz.ratio(self.sub, other.sub) > 90:\n",
    "            sub_eq = True\n",
    "        else:\n",
    "            self_sub_tokens = self.sub.split(' ')\n",
    "            other_sub_tokens = other.sub.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_sub_tokens)) and all(map(lambda x: x in w2v.wv, other_sub_tokens)):\n",
    "                self_sub_vec = np.mean([w2v.wv[x] for x in self_sub_tokens], axis=0)\n",
    "                other_sub_vec = np.mean([w2v.wv[x] for x in other_sub_tokens], axis=0)\n",
    "                sub_eq = spatial.distance.cosine(self_sub_vec, other_sub_vec) < 0.3\n",
    "            \n",
    "        rel_eq = False\n",
    "        if self.rel in other.rel or other.rel in self.rel:\n",
    "            rel_eq = True\n",
    "        elif self.rel == other.rel:\n",
    "            rel_eq = True\n",
    "        else:\n",
    "            self_rel_tokens = self.rel.split(' ')\n",
    "            other_rel_tokens = other.rel.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_rel_tokens)) and all(map(lambda x: x in w2v.wv, other_rel_tokens)):\n",
    "                self_rel_vec = np.mean([w2v.wv[x] for x in self_rel_tokens], axis=0)\n",
    "                other_rel_vec = np.mean([w2v.wv[x] for x in other_rel_tokens], axis=0)\n",
    "                rel_eq = spatial.distance.cosine(self_rel_vec, other_rel_vec) < 0.3\n",
    "\n",
    "        obj_eq = False\n",
    "        if self.obj in other.obj or other.obj in self.obj:\n",
    "            obj_eq = True\n",
    "        elif fuzz.ratio(self.obj, other.obj) > 90:\n",
    "            obj_eq = True\n",
    "        else:\n",
    "            self_obj_tokens = self.obj.split(' ')\n",
    "            other_obj_tokens = other.obj.split(' ')\n",
    "            if all(map(lambda x: x in w2v.wv, self_obj_tokens)) and all(map(lambda x: x in w2v.wv, other_obj_tokens)):\n",
    "                self_obj_vec = np.mean([w2v.wv[x] for x in self_obj_tokens], axis=0)\n",
    "                other_obj_vec = np.mean([w2v.wv[x] for x in other_obj_tokens], axis=0)\n",
    "                obj_eq = spatial.distance.cosine(self_obj_vec, other_obj_vec) < 0.3\n",
    "            \n",
    "        return sub_eq and rel_eq and obj_eq and self.sub_ent == other.sub_ent and self.obj_ent == other.obj_ent and self.rel_ent == other.rel_ent\n",
    "    def print(self):\n",
    "        print(f'{self.sub}({self.sub_ent}) >> {self.rel} >> {self.obj}({self.obj_ent})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457c5b0f-f980-4bca-94a4-40c42a861561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_ner(ann):\n",
    "    tokenized = [[ x.value for x in sent.token] for sent in ann.sentence]\n",
    "    predictions, raw_outputs = model.predict(tokenized, split_on_space=False);\n",
    "    return predictions, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2384e4-330e-4bbf-a9b6-7de9eb57e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_match(candidate, ioc_dic):\n",
    "    candidate = candidate.lower().strip()\n",
    "    if(candidate.startswith(\"cve\")):\n",
    "        return \"CVE\"\n",
    "    for _ in mitre_groups:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Actor\"\n",
    "    for _ in mitre_malwares:\n",
    "        if candidate.find(_) != -1:\n",
    "            return \"Known_Malware\"\n",
    "    for _ in programm_language:\n",
    "        if _ == candidate:\n",
    "            return \"Programming_Language\"\n",
    "    for _ in ioc_dic:\n",
    "        if _ in candidate:   \n",
    "            textobj = IOCParser(ioc_dic[_]);\n",
    "            results = textobj.parse();\n",
    "            if len(results) > 0:\n",
    "                return results[0].kind\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e46106b-d211-4e60-bcb1-914858b63bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_triple_tokens(tokens, ann, predictions, raws, ioc_dic):\n",
    "    redundant = ['PRP$', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'DT']\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ','NNS', 'NNPS']\n",
    "    sub_tokens = tokens\n",
    "    simplified_sub_tokens = []\n",
    "    sub_entity = ''\n",
    "    sub_ner_conf = 0\n",
    "    for sub_token in sub_tokens:\n",
    "        token_obj = ann.sentence[sub_token.sentenceIndex].token[sub_token.tokenIndex]\n",
    "        if token_obj.pos.startswith('NN'):\n",
    "            ner_tag = 'O'\n",
    "            try:\n",
    "                ner_tag = list(predictions[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0]\n",
    "                sub_ner_conf = max(scipy.special.softmax(list(raws[sub_token.sentenceIndex][sub_token.tokenIndex].values())[0][0]))\n",
    "            except:\n",
    "                pass\n",
    "            if ner_tag.startswith('B'):\n",
    "                sub_entity = ner_tag.split('-')[1]\n",
    "            if token_obj.pos in inflex:\n",
    "                simplified_sub_tokens.append(token_obj.lemma)\n",
    "            else:\n",
    "                simplified_sub_tokens.append(token_obj.word)\n",
    "        elif token_obj.pos not in redundant:\n",
    "            simplified_sub_tokens.append(token_obj.word)\n",
    "    if len(simplified_sub_tokens) == 0:\n",
    "        return None, None, None\n",
    "    else:\n",
    "        sub =  \" \".join(simplified_sub_tokens)\n",
    "        simple_ent = simple_match(sub, ioc_dic)\n",
    "        if simple_ent != '':\n",
    "            sub_entity = simple_ent \n",
    "            sub_ner_conf = 1\n",
    "    return sub, sub_entity, sub_ner_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d694db35-e5fc-4c33-b207-725bb2bca458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples_with_ner(ann, predictions, raws, ioc_dic, doc_id = None):\n",
    "    inflex = ['VBD', 'VBG', 'VBP', 'VBZ','NNS', 'NNPS']\n",
    "    extracted = list()\n",
    "    sub_counter = Counter()\n",
    "    obj_counter = Counter()\n",
    "    relation_counter = Counter()\n",
    "    for sent_ind, sent in enumerate(ann.sentence):\n",
    "        triples = sent.openieTriple\n",
    "        for triple in triples:\n",
    "            print(triple)\n",
    "            sub_tokens = triple.subjectTokens\n",
    "            sub, sub_entity, sub_ner_conf = process_triple_tokens(sub_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not sub:\n",
    "                continue\n",
    "            \n",
    "            obj_tokens = triple.objectTokens\n",
    "            obj, obj_entity, obj_ner_conf = process_triple_tokens(obj_tokens, ann, predictions, raws, ioc_dic)\n",
    "            if not obj:\n",
    "                continue\n",
    "\n",
    "            rel_tokens = triple.relationTokens\n",
    "            simplified_rel_tokens = []\n",
    "            for rel_token in rel_tokens:\n",
    "                token_obj = ann.sentence[rel_token.sentenceIndex].token[rel_token.tokenIndex]\n",
    "                if token_obj.pos != 'RB' and token_obj.lemma not in invalid_relations_set:\n",
    "                    if token_obj.pos in inflex:\n",
    "                        simplified_rel_tokens.append(token_obj.lemma)\n",
    "                    else:\n",
    "                        simplified_rel_tokens.append(token_obj.word)       \n",
    "            if len(simplified_rel_tokens) == 0:\n",
    "                print(\"no rel\")\n",
    "                continue\n",
    "            else:\n",
    "                rel = \" \".join(simplified_rel_tokens)\n",
    "                \n",
    "            for k,v in ioc_dic.items():\n",
    "                if k in sub:\n",
    "                    sub = sub.replace(k, v)\n",
    "                if k in obj:\n",
    "                    obj = obj.replace(k, v)\n",
    "                if k in rel:\n",
    "                    rel = rel.replace(k, v)\n",
    "                    \n",
    "            tri = Triple(sub, rel, obj, sub_ent=sub_entity, obj_ent=obj_entity, sub_ent_conf=sub_ner_conf, obj_ent_conf=obj_ner_conf, conf=triple.confidence)\n",
    "            tri.set_sent_ind(sent_ind)\n",
    "            tri.set_relative_sent_ind(sent_ind/len(ann.sentence))\n",
    "            tri.set_article_ref(doc_id)\n",
    "            tri.set_raw_triple(triple)\n",
    "            extracted.append(tri)\n",
    "            sub_counter.update([sub])\n",
    "            obj_counter.update([obj])\n",
    "            relation_counter.update([rel])\n",
    "    return extracted, (sub_counter, obj_counter), relation_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c478e4-73e0-4ac2-8b0b-64ac2b7ef90a",
   "metadata": {},
   "source": [
    "# Extract Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "056e6254-d4f3-4db2-9731-3807420c470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_ioc(text):\n",
    "    count = 0\n",
    "    dic = dict()\n",
    "    textobj = IOCParser(text);\n",
    "    results = textobj.parse();\n",
    "    for res in results:\n",
    "        sig = res.kind+str(count)\n",
    "        dic[sig] = res.value\n",
    "        count += 1\n",
    "        text = text.replace(res.value, sig)\n",
    "    return text, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6eb7535-f58c-4c5a-8610-4d6e8ac271a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../cluster/929博客聚类/929df_train.csv')\n",
    "df2 = pd.read_csv('../cluster/929博客聚类/929df_test.csv')\n",
    "df = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf1648b4-2fcd-48eb-8776-50787f11ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_pos = pd.read_csv('for_extract_raw_sents.csv', index_col=None)\n",
    "g = df.groupby('HDBSCAN的聚类结果')\n",
    "g_doc = articles_pos.groupby('hardcode_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da68637-1793-41de-9861-1e54d1a2928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_extracts = {}\n",
    "%%capture\n",
    "for cluster in [1, 2, 4, 6, 7, 8, 11, 14]:\n",
    "    cluster_extracts = {}\n",
    "    article_annotations = {}\n",
    "    with CoreNLPClient(properties={'annotators': 'tokenize,mwt,ssplit,pos,lemma,ner,parse,depparse,natlog,openie',\n",
    "                               'coref.algorithm' : 'neural',\n",
    "                               'ssplit.boundaryTokenRegex': '''<SEP>''',\n",
    "                               'openie.resolve_coref': False, \n",
    "                               'openie.triple.strict': True, \n",
    "                               'openie.triple.all_nominals':False,\n",
    "                               'openie.affinity_probability_cap': 1.0},\n",
    "                       memory='10G',\n",
    "                       threads=8,\n",
    "                       timeout=300000) as client:\n",
    "        cluster_name = cluster\n",
    "        texts = []\n",
    "        doc_ids = []\n",
    "        for doc_id in g.get_group(cluster).hardcode_id:\n",
    "            if doc_id in g_doc.groups:\n",
    "                texts.append('<SEP>'.join(g_doc.get_group(doc_id).line.values))\n",
    "                doc_ids.append(doc_id)\n",
    "\n",
    "        extracts = []\n",
    "        entity_c = []\n",
    "        rel_c = []\n",
    "        from_doc = []\n",
    "        for doc_id, text in tzip(doc_ids, texts):\n",
    "            text, ioc_dic = change_ioc(text)\n",
    "            text = nlp(text)._.coref_resolved;\n",
    "            ann = client.annotate(text)\n",
    "            ner, raw_out = bert_ner(ann)\n",
    "            extracted, entity_count, rel_count = extract_triples_with_ner(ann, ner, raw_out, ioc_dic, doc_id = doc_id)\n",
    "            extracts.append(extracted)\n",
    "            entity_c.append(entity_count)\n",
    "            rel_c.append(rel_count)\n",
    "            from_doc.append(doc_id)\n",
    "            article_annotations[doc_id] = ann\n",
    "\n",
    "        cluster_extracts[cluster_name] = extracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa90a0e-9573-41ed-b2dc-f49dafd7d519",
   "metadata": {},
   "source": [
    "# Triple Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767cf014-4d25-43c6-88c5-c108fdf60e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tactic_dic = {\n",
    "    0: 'Initial Access', \n",
    "    1: 'Execution', \n",
    "    2: 'Defense Evasion', \n",
    "    3: 'Command and Control', \n",
    "    4: 'Privilege Escalation', \n",
    "    5: 'Persistence', \n",
    "    6: 'Lateral Movement', \n",
    "    7: 'DataLeak', \n",
    "    8: 'Exfiltration', \n",
    "    9: 'Impact'\n",
    "}\n",
    "\n",
    "\n",
    "valid_ner = [\n",
    "         'Known_Malware',\n",
    "         'PRO',\n",
    "         'PERSON',\n",
    "         'filename',\n",
    "         'uri',\n",
    "         'md5',\n",
    "         'ACTOR',\n",
    "         'Known_Actor',\n",
    "         'sha256',\n",
    "         'URL',\n",
    "         'sha1',\n",
    "         'CVE',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a454f72b-b36e-4262-9d1f-85f94a52e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe(article_extracts):\n",
    "    seen_sent = dict()\n",
    "    unique = []\n",
    "    for triple in article_extracts:\n",
    "        sent_ind = triple.sent_ind\n",
    "        if sent_ind not in seen_sent:\n",
    "            seen_sent[sent_ind] = list()\n",
    "        \n",
    "        dup = False\n",
    "        for seen_triple in seen_sent[sent_ind]:\n",
    "            if seen_triple == triple:\n",
    "                if len(seen_triple.sub) >= len(triple.sub) and len(seen_triple.obj) >= len(triple.obj):\n",
    "                    seen_sent[sent_ind].remove(seen_triple)\n",
    "                    dup = False\n",
    "                else:\n",
    "                    seen_triple.set_count(seen_triple.count+1)\n",
    "                    dup = True\n",
    "                break\n",
    "                    \n",
    "        if not dup:\n",
    "            seen_sent[sent_ind].append(triple)\n",
    "    \n",
    "    for k, v in seen_sent.items():\n",
    "        unique += v\n",
    "        \n",
    "    return unique\n",
    "\n",
    "def filter_ner(triples):\n",
    "    filtered = []\n",
    "    for triple in triples:\n",
    "        if triple.sub_ent != '' and triple.sub_ent in valid_ner and triple.obj_ent != '' and triple.obj_ent in valid_ner:\n",
    "            filtered.append(triple)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter_conf(triples, behav_thresh=0.5):\n",
    "    filtered = []\n",
    "    for triple in triples:        \n",
    "        if triple.confidence < 0.8:\n",
    "            continue\n",
    "            \n",
    "        if triple.behave_conf < behav_thresh:\n",
    "            continue\n",
    "        \n",
    "        filtered.append(triple)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65dd39-bed7-49c3-ae02-46187daf270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_extracts = []\n",
    "for k,v in cluster_extracts.items():\n",
    "    all_extracts += v\n",
    "\n",
    "all_deduped = []\n",
    "for article_extracts in all_extracts:\n",
    "    all_deduped.append(dedupe(article_extracts))\n",
    "    \n",
    "all_triple = []\n",
    "for triples in all_deduped:\n",
    "    all_triple += triples\n",
    "    \n",
    "filtered = filter_conf(filter_ner(all_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab613943-3c38-435e-844c-8e62fee6fc16",
   "metadata": {},
   "source": [
    "# Knowledge Graph Building and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9bffd8-f61f-4167-856d-ec4225ad47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(triples):\n",
    "    g = nx.MultiDiGraph()\n",
    "    for triple in triples:\n",
    "        if triple.sub not in g.nodes:\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_node(triple.sub, appearance=1, ent=triple.sub_ent, mentions=s)\n",
    "        else:\n",
    "            g.nodes[triple.sub]['appearance'] += 1\n",
    "            g.nodes[triple.sub]['mentions'].append(triple)\n",
    "\n",
    "        if triple.obj not in g.nodes:\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_node(triple.obj, appearance=1, ent=triple.obj_ent, mentions=s)\n",
    "        else:\n",
    "            g.nodes[triple.obj]['appearance'] += 1\n",
    "            g.nodes[triple.obj]['mentions'].append(triple)\n",
    "\n",
    "        if not g.has_edge(triple.sub, triple.obj):\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_edge(triple.sub, triple.obj, appearance=1, relation=triple.rel, mentions=s)\n",
    "        elif g[triple.sub][triple.obj][0]['relation'] == triple.rel:\n",
    "            g[triple.sub][triple.obj][0]['appearance'] += 1\n",
    "            g[triple.sub][triple.obj][0]['mentions'].append(triple)\n",
    "        else:\n",
    "            s = []\n",
    "            s.append(triple)\n",
    "            g.add_edge(triple.sub, triple.obj, appearance=1, relation=triple.rel, mentions=s)\n",
    "    return g\n",
    "\n",
    "def extract_subgraph(g, nodes):\n",
    "    sg = g.subgraph(nodes)\n",
    "    sg_tactics = set()\n",
    "    sg_articles = set()\n",
    "    sg_linking_nodes = set()\n",
    "    \n",
    "    edge_labels = {}\n",
    "    edge_mentions = []\n",
    "    node2article = dict()\n",
    "    for edge in sg.edges(data=True):\n",
    "        mentions = edge[2]['mentions']\n",
    "        sg_articles.update(map(lambda x: x.article_id, mentions))\n",
    "        for mention in mentions:\n",
    "            for tac in mention.tactic:\n",
    "                sg_tactics.add(tac)\n",
    "        if edge[0] not in node2article:\n",
    "            node2article[edge[0]] = set()\n",
    "        node2article[edge[0]].update(map(lambda x: x.article_id, mentions))\n",
    "        if edge[1] not in node2article:\n",
    "            node2article[edge[1]] = set()\n",
    "        node2article[edge[1]].update(map(lambda x: x.article_id, mentions))\n",
    "        \n",
    "    for k, v in node2article.items():\n",
    "        if len(v)>1:\n",
    "            sg_linking_nodes.add(k)\n",
    "            \n",
    "    return sg, sg_tactics, sg_articles, sg_linking_nodes\n",
    "\n",
    "def draw_graph(g, scheme=False, legend=False):\n",
    "    node_deg = nx.degree(g)\n",
    "    layout = nx.spring_layout(g, k=2, iterations=80)\n",
    "    plt.figure(num=None, figsize=(20, 10), dpi=80)\n",
    "\n",
    "    if scheme:\n",
    "        node_labels = {node[0] : f\"{node[0]}\" for node in g.nodes(data=True)}\n",
    "    else:\n",
    "        node_labels = {node[0] : f\"{node[0]}\\n({node[1]['ent']})\" for node in g.nodes(data=True)}\n",
    "    nx.draw_networkx(\n",
    "        g,\n",
    "        node_size=[int(deg[1]) * 1000 for deg in node_deg],\n",
    "        arrowsize=10,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        node_shape=\"s\",\n",
    "        bbox=dict(facecolor=\"skyblue\", edgecolor='black', boxstyle='round,pad=0.1'),\n",
    "        node_color='white',\n",
    "        font_size=13,\n",
    "        with_labels=True,\n",
    "        labels = node_labels,\n",
    "        )\n",
    "\n",
    "    edge_labels = {}\n",
    "    edge_mentions = []\n",
    "    for edge in g.edges(data=True):\n",
    "        mentions = edge[2]['mentions']\n",
    "        mention_articles = ','.join(map(lambda x: str(x.article_id), mentions))\n",
    "        mention_tactics = set()\n",
    "        for mention in mentions:\n",
    "            if len(mention.tactic)>0:\n",
    "                for tac in mention.tactic: \n",
    "                    mention_tactics.add(tactic_dic[tac])\n",
    "        mention_tactics = '('+','.join(mention_tactics)+')'\n",
    "        \n",
    "        edge_labels[(edge[0], edge[1])] = '-'.join([mention_tactics, edge[2]['relation'], mention_articles])\n",
    "        edge_mentions.append(mentions)\n",
    "    edge_mentions = [item for sublist in edge_mentions for item in sublist]\n",
    "    descriptions = set([x.article_id for x in edge_mentions])\n",
    "    \n",
    "    texts = ['::'.join([str(x), articles.loc[x]['title'], articles.loc[x]['date']]) for x in descriptions]\n",
    "    nx.draw_networkx_edge_labels(g, \n",
    "                                 pos=layout, \n",
    "                                 edge_labels=edge_labels,\n",
    "                                 font_color='red',\n",
    "                                 font_size=12)\n",
    "    if legend:\n",
    "        plt.legend(texts, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefdf68-21de-4535-a64d-1f47cf3f62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = build_graph(filtered)\n",
    "draw_graph(kg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
